---
permalink: /
# title: "About Me"
# excerpt: "About me"
author_profile: true
redirect_from: 
  - /home/
  - /home.html
---

{% include base_path %}

# About Me
I'm a senior research scientist and lead the Voice Synthesis team at [Naver Cloud](https://navercloudcorp.com/lang/en/), Korea (from Jan 2023; [Naver Corporation](https://www.navercorp.com/en) from Mar 2017 to Dec 2022). I'm also an adjunct professor in [Artificial Intelligence Institute](https://aiis.snu.ac.kr/eng/) at [Seoul National University](https://en.snu.ac.kr/index.html), Seoul, Korea (from Aug 2022).

I received my Ph.D. degree in department of Electrical and Electronics at [Yonsei University](https://www.yonsei.ac.kr/en_sc/index.jsp), Seoul, Korea. During my Ph.D., I served my internships at [Microsoft Research Asia](https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/), Beijing, China and [Qualcomm Technologies Inc.](https://www.qualcomm.com/company?#about), San Diego, CA.

My research interests include speech synthesis and its real-world applications. Specifically, I develop a high-quality TTS api for cloud services ([Clova Voice Pro](https://www.ncloud.com/product/aiService/clovaVoice), [Clova Dubbing](https://clovadubbing.naver.com/)),
an automatic TTS modeling with smartphone recordings ([Voice Maker](https://clovadubbing.naver.com/voicemaker)), and a hybrid TTS system combining deep learning and unit-selection TTS models ([Clova AI speaker](https://blog.naver.com/clova_ai/221409341851), [Naver Maps navigation](https://blog.naver.com/naver_map/222109060982), [Naver News anchor](https://blog.naver.com/clova_ai/221981676372)).

If you are interested in me, feel free to [contact me]({{ base_path }}/contacts.html).

<small><i class="fa fa-download" aria-hidden="true"></i> Download my [CV]({{base_path}}/files/CV_EunwooSong.pdf)</small>

***
# Recent Publications
- RapFlow-TTS: Rapid and high-fidelity text-to-speech with improved consistency flow matching [[demo](https://tts-demo.github.io/rap.github.io/)]  
  <small>Hyun Joon Park, Jeongmin Liu, Jin Sob Kim, Jeong Yeol Yang, Sung Won Han, <strong style="color:orange">Eunwoo Song</strong></small>  
  <small>Proc. Interspeech, 2025 (in press).</small>  
  
- Paralinguistics-aware speech-empowered large language models for natural conversation [[paper](https://sewplay.github.io/files/papers/2024/neurips_7916.pdf)][[demo](https://unifiedsdm.github.io/)]  
  <small>Heeseung Kim, Soonshin Seo, Kyeongseok Jeong, Ohsung Kwon, Soyoon Kim, Jungwhan Kim, Jaehong Lee, <strong style="color:orange">Eunwoo Song</strong>, Myungwoo Oh, Jung-Woo Ha, Sungroh Yoon, Kang Min Yoo</small>  
  <small>Proc. NeurIPS, 2024, pp. 131072-131103.</small>  
  
- Enhancing multilingual TTS with voice conversion based data augmentation and posterior embedding [[paper](https://sewplay.github.io/files/papers/2024/icassp_0012186.pdf)][[demo](https://christophyoon.github.io/MMV-TTS/)]  
  <small>Hyun-Wook Yoon, Jin-Seob Kim, Ryuichi Yamamoto, Ryo Terashima, Chan-Ho Song, Jae-Min Kim, <strong style="color:orange">Eunwoo Song</strong></small>  
  <small>Proc. ICASSP, 2024, pp. 12186-12190.</small>  
  
- Pruning self-attention for zero-shot multi-speaker text-to-speech [[paper](https://sewplay.github.io/files/papers/2023/IS_1301.pdf)][[demo](https://hcy71o.github.io/SparseTTS-demo/)]  
  <small>Hyungchan Yoon, Changhwan Kim, <strong style="color:orange">Eunwoo Song</strong>, Hyun-Wook Yoon, Hong-Goo Kang</small>  
  <small>Proc. INTERSPEECH, 2023, pp. 4299-4303.</small>  
  
- Period VITS: Variational inference with explicit pitch modeling for end-to-end emotional speech synthesis [[paper](https://sewplay.github.io/files/papers/2023/icassp_1241.pdf)][[demo](https://yshira116.github.io/period_vits_demo/)]  
  <small>Yuma Shirahata, Ryuichi Yamamoto, <strong style="color:orange">Eunwoo Song</strong>, Ryo Terashima, Jae-Min Kim, Kentaro Tachibana</small>  
  <small>Proc. ICASSP, 2023, pp. 4299-4303.</small>  
  
- HierSpeech: Bridging the gap between text and speech by hierarchical variational inference using self-supervised representations for speech synthesis [[paper](https://sewplay.github.io/files/papers/2022/neurips_54658.pdf)][[demo](https://sh-lee-prml.github.io/hierspeech-demo/)]  
  <small>Sang-Hoon Lee, Seung-Bin Kim, Ji-Hyun Lee, <strong style="color:orange">Eunwoo Song</strong>, Min-Jae Hwang, Seong-Whan Lee</small>  
  <small>Proc. NeurIPS, 2022, pp. 16624-16636.</small>  

- TTS-by-TTS 2: Data-selective augmentation for neural speech synthesis using ranking support vector machine with variational autoencoder [[paper](https://sewplay.github.io/files/papers/2022/IS_10134.pdf)][[demo](https://sewplay.github.io/demos/txt2/)]  
  <small><strong style="color:orange">Eunwoo Song</strong>, Ryuichi Yamamoto, Ohsung Kwon, Chan-Ho Song, Min-Jae Hwang, Suhyeon Oh, Hyun-Wook Yoon, Jin-Seob Kim, Jae-Min Kim</small>  
  <small>Proc. INTERSPEECH, 2022, pp. 1941-1945.</small>  
  
  <small>[[See more]]({{ base_path }}/publications/)</small>
  
*** 
# Recent Talks
- AI Human: Large-scale text-to-speech applications
  [[Slides](https://sewplay.github.io/files/talks/2025/20250122_snu.pdf)]  
  <small>SNU, Jan 2025</small>  

- Speech synthesis and applications
  [[Slides](https://sewplay.github.io/files/talks/2023/20231201_snu.pdf)]  
  <small>SNU, Dec 2023</small>  

- Parallel waveform synthesis
  [[Slides](https://sewplay.github.io/files/talks/2022/20220913_sr.pdf)]  
  <small>Samsung Research, Sep 2022</small>  

- Data-selective TTS augmentation
  [[Slides](https://sewplay.github.io/files/talks/2022/20220712_naver_tts.pdf)]  
  <small>Naver Engineering Day, Jul 2022</small>  

  <small>[[See more]]({{ base_path }}/talks/)</small>
